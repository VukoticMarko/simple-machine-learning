{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c363896e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26.2\n",
      "3.9.4 (tags/v3.9.4:1f2e308, Apr  6 2021, 13:40:21) [MSC v.1928 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "print(gym.__version__)\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "219bda36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\blast\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 0] - Mean survival time over last 100 episodes was 12.0 ticks.\n",
      "[Episode 20] - Mean survival time over last 100 episodes was 10.333333333333334 ticks.\n",
      "[Episode 40] - Mean survival time over last 100 episodes was 24.048780487804876 ticks.\n",
      "[Episode 60] - Mean survival time over last 100 episodes was 23.081967213114755 ticks.\n",
      "[Episode 80] - Mean survival time over last 100 episodes was 24.814814814814813 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 22.59 ticks.\n",
      "[Episode 120] - Mean survival time over last 100 episodes was 24.48 ticks.\n",
      "[Episode 140] - Mean survival time over last 100 episodes was 24.45 ticks.\n",
      "[Episode 160] - Mean survival time over last 100 episodes was 27.63 ticks.\n",
      "[Episode 180] - Mean survival time over last 100 episodes was 27.49 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 28.19 ticks.\n",
      "[Episode 220] - Mean survival time over last 100 episodes was 28.06 ticks.\n",
      "[Episode 240] - Mean survival time over last 100 episodes was 26.06 ticks.\n",
      "[Episode 260] - Mean survival time over last 100 episodes was 29.65 ticks.\n",
      "[Episode 280] - Mean survival time over last 100 episodes was 37.79 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 37.72 ticks.\n",
      "[Episode 320] - Mean survival time over last 100 episodes was 42.75 ticks.\n",
      "[Episode 340] - Mean survival time over last 100 episodes was 39.78 ticks.\n",
      "[Episode 360] - Mean survival time over last 100 episodes was 39.12 ticks.\n",
      "[Episode 380] - Mean survival time over last 100 episodes was 27.76 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 26.54 ticks.\n",
      "[Episode 420] - Mean survival time over last 100 episodes was 27.19 ticks.\n",
      "[Episode 440] - Mean survival time over last 100 episodes was 36.3 ticks.\n",
      "[Episode 460] - Mean survival time over last 100 episodes was 40.08 ticks.\n",
      "[Episode 480] - Mean survival time over last 100 episodes was 58.15 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 70.83 ticks.\n",
      "[Episode 520] - Mean survival time over last 100 episodes was 82.74 ticks.\n",
      "[Episode 540] - Mean survival time over last 100 episodes was 97.52 ticks.\n",
      "[Episode 560] - Mean survival time over last 100 episodes was 117.38 ticks.\n",
      "[Episode 580] - Mean survival time over last 100 episodes was 122.44 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 142.02 ticks.\n",
      "[Episode 620] - Mean survival time over last 100 episodes was 164.4 ticks.\n",
      "[Episode 640] - Mean survival time over last 100 episodes was 172.16 ticks.\n",
      "[Episode 660] - Mean survival time over last 100 episodes was 168.49 ticks.\n",
      "[Episode 680] - Mean survival time over last 100 episodes was 175.67 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 178.09 ticks.\n",
      "Ran 711 episodes. Solved after 611 trials\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "611"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Training Parameters\n",
    "n_episodes = 1000\n",
    "n_win_ticks = 195\n",
    "max_env_steps = None\n",
    "gamma = 1.0\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "alpha = 0.01\n",
    "alpha_decay = 0.01\n",
    "batch_size = 64\n",
    "monitor = False\n",
    "quiet = False\n",
    "\n",
    "# Environment Parameters\n",
    "memory = deque(maxlen=100000)\n",
    "env = gym.make('CartPole-v1', render_mode = 'human')  # Switch to CartPole-v1 for compatibility\n",
    "if max_env_steps is not None:\n",
    "    env.max_episode_steps = max_env_steps\n",
    "\n",
    "# Model Definition\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(4,)))\n",
    "model.add(Dense(24, input_dim=4, activation='relu'))  # Input dimension matches state size for CartPole-v1\n",
    "model.add(Dense(48, activation='relu'))\n",
    "model.add(Dense(2, activation='linear'))  # Use 'linear' for the output layer\n",
    "\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate=alpha,\n",
    "    decay_steps=100000,  # Define the decay steps\n",
    "    decay_rate=0.96,     # Define the decay rate\n",
    "    staircase=True       # True for step-wise decay, False for continuous decay\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=Adam(learning_rate=lr_schedule)  # Use learning rate schedule\n",
    ")\n",
    "\n",
    "def remember(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "def choose_action(state, epsilon):\n",
    "    return env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(model.predict(state, verbose=0))\n",
    "\n",
    "def get_epsilon(t):\n",
    "    return max(epsilon_min, min(epsilon, 1.0 - math.log10((t + 1) * epsilon_decay)))\n",
    "\n",
    "def preprocess_state(state):\n",
    "    state = np.array(state, dtype=np.float32)\n",
    "    return np.reshape(state, [1, len(state)])  # Dynamic state length handling\n",
    "\n",
    "def replay(batch_size, epsilon):\n",
    "    x_batch, y_batch = [], []\n",
    "    minibatch = random.sample(memory, min(len(memory), batch_size))\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        y_target = model.predict(state, verbose=0)\n",
    "        y_target[0][action] = reward if done else reward + gamma * np.max(model.predict(next_state, verbose=0)[0])\n",
    "        x_batch.append(state[0])\n",
    "        y_batch.append(y_target[0])\n",
    "\n",
    "    model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
    "\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "def run():\n",
    "    scores = deque(maxlen=100)\n",
    "\n",
    "    for e in range(n_episodes):\n",
    "        state = preprocess_state(env.reset()[0])  # Adjust for new reset() output structure\n",
    "        done = False\n",
    "        i = 0\n",
    "        while not done:\n",
    "            action = choose_action(state, get_epsilon(e))\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = preprocess_state(next_state)\n",
    "            done = terminated or truncated\n",
    "            env.render()\n",
    "            remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            i += 1\n",
    "\n",
    "        scores.append(i)\n",
    "        mean_score = np.mean(scores)\n",
    "        if mean_score >= n_win_ticks and e >= 100:\n",
    "            if not quiet:\n",
    "                print(f'Ran {e} episodes. Solved after {e - 100} trials')\n",
    "            return e - 100\n",
    "        if e % 20 == 0 and not quiet:\n",
    "            print(f'[Episode {e}] - Mean survival time over last 100 episodes was {mean_score} ticks.')\n",
    "\n",
    "        replay(batch_size, get_epsilon(e))\n",
    "\n",
    "    if not quiet:\n",
    "        print(f'Did not solve after {e} episodes')\n",
    "    return e\n",
    "\n",
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
