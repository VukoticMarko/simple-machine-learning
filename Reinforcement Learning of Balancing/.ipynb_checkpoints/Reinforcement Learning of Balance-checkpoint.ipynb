{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c363896e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26.2\n",
      "3.9.4 (tags/v3.9.4:1f2e308, Apr  6 2021, 13:40:21) [MSC v.1928 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "print(gym.__version__)\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "219bda36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\blast\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 0] - Mean survival time over last 100 episodes was 12.0 ticks.\n",
      "[Episode 20] - Mean survival time over last 100 episodes was 10.333333333333334 ticks.\n",
      "[Episode 40] - Mean survival time over last 100 episodes was 24.048780487804876 ticks.\n",
      "[Episode 60] - Mean survival time over last 100 episodes was 23.081967213114755 ticks.\n",
      "[Episode 80] - Mean survival time over last 100 episodes was 24.814814814814813 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 22.59 ticks.\n",
      "[Episode 120] - Mean survival time over last 100 episodes was 24.48 ticks.\n",
      "[Episode 140] - Mean survival time over last 100 episodes was 24.45 ticks.\n",
      "[Episode 160] - Mean survival time over last 100 episodes was 27.63 ticks.\n",
      "[Episode 180] - Mean survival time over last 100 episodes was 27.49 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 28.19 ticks.\n",
      "[Episode 220] - Mean survival time over last 100 episodes was 28.06 ticks.\n",
      "[Episode 240] - Mean survival time over last 100 episodes was 26.06 ticks.\n",
      "[Episode 260] - Mean survival time over last 100 episodes was 29.65 ticks.\n",
      "[Episode 280] - Mean survival time over last 100 episodes was 37.79 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 37.72 ticks.\n",
      "[Episode 320] - Mean survival time over last 100 episodes was 42.75 ticks.\n",
      "[Episode 340] - Mean survival time over last 100 episodes was 39.78 ticks.\n",
      "[Episode 360] - Mean survival time over last 100 episodes was 39.12 ticks.\n",
      "[Episode 380] - Mean survival time over last 100 episodes was 27.76 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 26.54 ticks.\n",
      "[Episode 420] - Mean survival time over last 100 episodes was 27.19 ticks.\n",
      "[Episode 440] - Mean survival time over last 100 episodes was 36.3 ticks.\n",
      "[Episode 460] - Mean survival time over last 100 episodes was 40.08 ticks.\n",
      "[Episode 480] - Mean survival time over last 100 episodes was 58.15 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 70.83 ticks.\n",
      "[Episode 520] - Mean survival time over last 100 episodes was 82.74 ticks.\n",
      "[Episode 540] - Mean survival time over last 100 episodes was 97.52 ticks.\n",
      "[Episode 560] - Mean survival time over last 100 episodes was 117.38 ticks.\n",
      "[Episode 580] - Mean survival time over last 100 episodes was 122.44 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 142.02 ticks.\n",
      "[Episode 620] - Mean survival time over last 100 episodes was 164.4 ticks.\n",
      "[Episode 640] - Mean survival time over last 100 episodes was 172.16 ticks.\n",
      "[Episode 660] - Mean survival time over last 100 episodes was 168.49 ticks.\n",
      "[Episode 680] - Mean survival time over last 100 episodes was 175.67 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 178.09 ticks.\n",
      "Ran 711 episodes. Solved after 611 trials\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "611"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Training Parameters\n",
    "n_episodes = 1000  # Number of episodes to run the training\n",
    "n_win_ticks = 195  # Number of ticks required to consider the task solved\n",
    "max_env_steps = None  # Maximum number of steps per episode\n",
    "gamma = 1.0  # Discount factor for future rewards\n",
    "epsilon = 1.0  # Exploration rate (initially high to explore more)\n",
    "epsilon_min = 0.01  # Minimum value for exploration rate\n",
    "epsilon_decay = 0.995  # Decay rate for epsilon (exploration) over time\n",
    "alpha = 0.01  # Learning rate for the model optimizer\n",
    "alpha_decay = 0.01  # Decay for the learning rate\n",
    "batch_size = 64  # Size of the minibatch for training\n",
    "monitor = False  # Whether to monitor the environment (rendering)\n",
    "quiet = False  # Whether to print progress or not\n",
    "\n",
    "# Environment Parameters\n",
    "memory = deque(maxlen=100000)  # Replay buffer to store experience\n",
    "env = gym.make('CartPole-v1', render_mode='human')  # Create the CartPole environment\n",
    "if max_env_steps is not None:\n",
    "    env.max_episode_steps = max_env_steps  # Set the max number of steps per episode if defined\n",
    "\n",
    "# Model Definition\n",
    "model = Sequential()  # Initialize the neural network model\n",
    "model.add(Input(shape=(4,)))  # Define the input layer with 4 features for CartPole-v1 state\n",
    "model.add(Dense(24, activation='relu'))  # Hidden layer with 24 neurons and ReLU activation\n",
    "model.add(Dense(48, activation='relu'))  # Another hidden layer with 48 neurons\n",
    "model.add(Dense(2, activation='linear'))  # Output layer with 2 neurons (representing actions)\n",
    "\n",
    "# Learning Rate Schedule (decay over time)\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate=alpha,\n",
    "    decay_steps=100000,  # Define how frequently to decay the learning rate\n",
    "    decay_rate=0.96,     # Define the decay rate for learning rate\n",
    "    staircase=True       # Whether to apply step-wise decay\n",
    ")\n",
    "\n",
    "# Compile the model with Mean Squared Error loss function and Adam optimizer\n",
    "model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=Adam(learning_rate=lr_schedule)  # Use learning rate schedule in optimizer\n",
    ")\n",
    "\n",
    "# Store the experience (state, action, reward, next state, done flag)\n",
    "def remember(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "# Choose an action based on the epsilon-greedy policy\n",
    "def choose_action(state, epsilon):\n",
    "    # If exploring (epsilon is high), pick a random action\n",
    "    # If exploiting (epsilon is low), pick the action with the highest predicted reward\n",
    "    return env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(model.predict(state, verbose=0))\n",
    "\n",
    "# Calculate the epsilon value for exploration vs exploitation at time step t\n",
    "def get_epsilon(t):\n",
    "    # Decay epsilon over time, ensuring it never goes below epsilon_min\n",
    "    return max(epsilon_min, min(epsilon, 1.0 - math.log10((t + 1) * epsilon_decay)))\n",
    "\n",
    "# Preprocess the state before feeding it into the model (reshape it to fit model input)\n",
    "def preprocess_state(state):\n",
    "    state = np.array(state, dtype=np.float32)  # Ensure state is a numpy array of type float32\n",
    "    return np.reshape(state, [1, len(state)])  # Reshape state to match model input\n",
    "\n",
    "# Experience replay: sample a batch from memory and train on it\n",
    "def replay(batch_size, epsilon):\n",
    "    x_batch, y_batch = [], []  # Initialize batches for input and target values\n",
    "    minibatch = random.sample(memory, min(len(memory), batch_size))  # Sample random minibatch from memory\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        y_target = model.predict(state, verbose=0)  # Predict the current Q-values for the state\n",
    "        # If done, set Q-value for the action to the reward, else add the discounted future Q-value\n",
    "        y_target[0][action] = reward if done else reward + gamma * np.max(model.predict(next_state, verbose=0)[0])\n",
    "        x_batch.append(state[0])  # Add state to the batch\n",
    "        y_batch.append(y_target[0])  # Add target Q-value to the batch\n",
    "\n",
    "    # Train the model on the batch\n",
    "    model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
    "\n",
    "    # Decay epsilon for exploration (so we explore less as training progresses)\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "# Main training loop\n",
    "def run():\n",
    "    scores = deque(maxlen=100)  # Store scores from the last 100 episodes\n",
    "\n",
    "    for e in range(n_episodes):  # Run for the specified number of episodes\n",
    "        state = preprocess_state(env.reset()[0])  # Reset the environment and preprocess the initial state\n",
    "        done = False  # Initialize done flag\n",
    "        i = 0  # Ticks (steps) within the current episode\n",
    "        while not done:  # Loop through each step within the episode\n",
    "            action = choose_action(state, get_epsilon(e))  # Choose an action using epsilon-greedy policy\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)  # Take the action in the environment\n",
    "            next_state = preprocess_state(next_state)  # Preprocess the next state\n",
    "            done = terminated or truncated  # Check if the episode is done\n",
    "            env.render()  # Render the environment (visualize the agent's actions)\n",
    "            remember(state, action, reward, next_state, done)  # Store experience in memory\n",
    "            state = next_state  # Update the state to the next state\n",
    "            i += 1  # Increment tick count\n",
    "\n",
    "        scores.append(i)  # Append the current episode score (survival time)\n",
    "        mean_score = np.mean(scores)  # Calculate the average score over the last 100 episodes\n",
    "        if mean_score >= n_win_ticks and e >= 100:  # Check if the task is solved\n",
    "            if not quiet:\n",
    "                print(f'Ran {e} episodes. Solved after {e - 100} trials')  # Print success message\n",
    "            return e - 100  # Return the episode when the task was solved\n",
    "        if e % 20 == 0 and not quiet:  # Print progress every 20 episodes\n",
    "            print(f'[Episode {e}] - Mean survival time over last 100 episodes was {mean_score} ticks.')\n",
    "\n",
    "        replay(batch_size, get_epsilon(e))  # Perform experience replay\n",
    "\n",
    "    if not quiet:\n",
    "        print(f'Did not solve after {e} episodes')  # If the task is not solved, print this message\n",
    "    return e  # Return the number of episodes run if not solved\n",
    "\n",
    "run()  # Start the training loop\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
